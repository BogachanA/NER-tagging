# -*- coding: utf-8 -*-
"""Project 2 - CS 445 - bogachanarslan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h3C-Qron79T7gphK1MzVuHRTZBV0NbHE
"""

!pip install -U 'scikit-learn<0.24'
!pip install sklearn-crfsuite

# YOU NEED TO RESTART THE RUNTIME!!!

# Run this cell to mount your drive to this notebook in order to read the datasets
from google.colab import drive
drive.mount('/content/drive')

import os
import re
import json
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings("ignore")

"""## Read Dataset"""

# Put the folder path where the datasets are located
PATH = "/content/drive/MyDrive/445Project2/"

# function to read data, return list of tuples each tuple represents a token contains word, pos tag, chunk tag, and ner tag
def read_data(filename) -> list:
  tuples = []
  currSent = []
  for l in open(filename,'r').readlines():
    if l.strip() == "":
      tuples.append(currSent)
      currSent = []
    else: 
      currSent.append(tuple(l.strip().split(" ")))
  if len(currSent) !=0:
    tuples.append(currSent)
  #tuples = [tuple(l.strip().split(" ")) for l in open(filename,'r').readlines() if l.strip() != "" ]
  return tuples

# read data with your custom function
train_data = read_data(PATH + "train.txt")
val_data = read_data(PATH + "valid.txt")
test_data = read_data(PATH + "test.txt")

print(len(train_data))
print(len(val_data))
print(len(test_data))

"""# Create Gazetteer"""

# load wikipedia pages
W_PATH = PATH + "wikipedia_pages/"

count = 0
gazetteer = set()
for filename in os.listdir(W_PATH):
  f = open(W_PATH + filename, 'r')
  j = json.load(f)
  #print("Reading:",j["title"])
  matches = re.findall('a href=\"[A-Z][^\s]+\"&gt;(?P<link>[A-Z][\w ]*)&lt;',j["text"])
  gazetteer.update(matches)
  gazetteer.add(j["title"])
  count +=1
  #print(count,end=" ")
  #if count > 4: break

# print the size of your gazetteer
print(len(gazetteer))

"""# Models

## Conditional Random Fields (CRF)

### Extract features for CRF
"""

import nltk
from nltk.stem import PorterStemmer

nltk.download('stopwords')
from nltk.corpus import stopwords

import sklearn_crfsuite
from sklearn.metrics import make_scorer
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics
from sklearn.model_selection import GridSearchCV

# create a function to extract features for each token

def token2features(sentence: list, idx: int) -> dict:
  word = sentence[idx][0]

  features = {
      'bias': 1.0,
      'stem': stem(word),
      'postag': sentence[idx][1],
      'chunktag': sentence[idx][2],
      'shortShape': shortShape(word),
      'containsNum': containsNum(word),
      'startUppercase': startUppercase(word),
      'wiShape': wiShape(word),
      'containsHyphen':containsHyphen(word),
      'upperHyphenDigit': upperHyphenDigit(word),
      'allUpper': allUpper(word),
      'isStopWord':isStopWord(word),
      'upper':upper(word),
      'inGazetteer': inGazetteer(word),
      'prefix': word[:3],
      'suffix': word[-3:]
  }
  if idx > 0:
    leftword = sentence[idx-1][0]
    features.update({
        'left':leftword,
        'left-postag': sentence[idx-1][1],
        'left-chunktag': sentence[idx-1][2],
        'left-wiShape':wiShape(leftword),
        'left-shortShape': shortShape(leftword),
        'beginning': False
    })
  else:
    features['beginning'] = True
  
  if idx == len(sentence)-1:
    features['end'] = True
  else:
    rightword = sentence[idx+1][0]
    features.update({
        'right':rightword,
        'right-postag': sentence[idx+1][1],
        'right-chunktag': sentence[idx+1][2],
        'right-wiShape':wiShape(rightword),
        'right-shortShape': shortShape(rightword),
        'end': False
    })

  return features

# Feature functions

ps=PorterStemmer()
def stem(word):
  return ps.stem(word)

def startUppercase(word):
  return 'A'< word[0] < 'Z'

def wiShape(word):
  regs = ["[A-Z]", "[a-z]", "[0-9]"]
  subs = ["X","x","d"]

  for i in range(len(regs)):
    word = re.sub(regs[i],subs[i],word)
  return word

def shortShape(word):
  regs = ["[A-Z]+", "[a-z]+", "[0-9]+"]
  subs = ["X","x","d"]

  for i in range(len(regs)):
    word = re.sub(regs[i],subs[i],word)
  return word

def containsNum(word):
  for i in range(len(word)):
    if word[i].isdigit(): return True
  return False

def containsHyphen(word):
  for i in range(len(word)):
    if word[i]=='-': return True
  return False

def allUpper(word):
  return word.isupper()

def upperHyphenDigit(word):
  return allUpper(word) and containsNum(word) and containsHyphen(word)

def isStopWord(word):
  return word in stopwords.words('english')

def upper(word):
  return word.upper()

def inGazetteer(word):
  global gazetteer
  return word in gazetteer

print(allUpper("ShdDDHD4D4"))

# define function to process each token given a sentence
def sent2features(sentence: list) -> list:
  return [token2features(sentence, i) for i in range(len(sentence))]

# get named entity labels from the sentence
def sent2labels(sentence: list) -> list:
  return [s[3] for s in sentence]

# prepare inputs and labels
train_sents = [sent2features(s) for s in train_data]
val_sents = [sent2features(s) for s in val_data]
test_sents = [sent2features(s) for s in test_data]

train_labels = [sent2labels(s) for s in train_data]
val_labels = [sent2labels(s) for s in val_data]
test_labels = [sent2labels(s) for s in test_data]

# calculate f1-score and classification report for test using sklearn_crfsuite.metrics class
train_sents[4][2]

# start from the stem of the token and add features one by one and train a new model with each feature that you add

keys = set(train_sents[0][0].keys())
keys.update(train_sents[4][2].keys())
keys = list(keys)

# CRF fit
import copy

crf = sklearn_crfsuite.CRF(algorithm='lbfgs',
          c1=0.1,
          c2=0.1,
          max_iterations=100,
          all_possible_transitions=False)

tf_copy = copy.deepcopy(train_sents)
for j in range(len(tf_copy)):
  for k in range(len(tf_copy[j])):
    tf_copy[j][k] = {}

results = {'Features': [], 'F1 Score':[], 'Precision':[],'Recall':[]}

for i in range(len(keys)):

  for j in range(len(tf_copy)):
    for k in range(len(tf_copy[j])):
      if keys[i] in train_sents[j][k].keys():
        tf_copy[j][k].update({keys[i]:train_sents[j][k][keys[i]]})

  crf.fit(tf_copy, train_labels)

  y_pred = crf.predict(val_sents)
  f1 = metrics.flat_f1_score(val_labels, y_pred,
                      average='weighted', labels=list(crf.classes_))
  pre =  metrics.flat_precision_score(val_labels, y_pred,
                      average='weighted', labels=list(crf.classes_))
  re = metrics.flat_recall_score(val_labels, y_pred,
                      average='weighted', labels=list(crf.classes_))
  
  results['Features'].append(keys[i])
  results['F1 Score'].append(f1)
  results['Precision'].append(pre)
  results['Recall'].append(re)


df = pd.DataFrame(results)
df

# display the classification report for the best model

"""## Recurrent Neural Network (RNN)"""

import tensorflow as tf
import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from keras.models import Model, Input, Sequential
from keras.layers import Dense, Flatten, Embedding, Input, Dropout, LSTM, TimeDistributed, Bidirectional
from tensorflow.keras.layers import add
from keras.callbacks import ModelCheckpoint

from gensim.models import Word2Vec
import gensim.downloader as api

!pip install seqeval
from seqeval.metrics import f1_score, precision_score, recall_score, classification_report

# find unique labels and create dictionary to map each label to a unique integer value
all_labels = set()
for t in train_data:
  #print(t)
  for word in t:
    all_labels.add(word[3])

label_vals = {}
for i, l in enumerate(all_labels):
  label_vals[l] = i
label_vals["Other"] = len(label_vals)
print(label_vals)

# preprare your dataset for RNN classifier (you need to add padding to labels as well)
#Tokenize and pad words
tokenizer = Tokenizer()

train_words = [[j[0] for j in w] for w in train_data ]
val_words = [[j[0] for j in w] for w in val_data ]
print(train_words[:4])
tokenizer.fit_on_texts(train_words)
train_seq  = tokenizer.texts_to_sequences(train_words) 
val_seq = tokenizer.texts_to_sequences(val_words)

#padding to prepare sequences of same length
train_seq_pad  = pad_sequences(train_seq, maxlen=100,padding="post")
val_seq_pad = pad_sequences(val_seq, maxlen=100,padding="post")

word_vec_size = 100

word2ind = tokenizer.word_index
n_unique = len(word2ind)
#print(train_seq_pad[:4])

#padding the labels 
tokenizer = Tokenizer()

train_labels = [[j[3] for j in w] for w in train_data ]
val_labels = [[j[3] for j in w] for w in val_data ]
#print(train_labels[:4])
tokenizer.fit_on_texts(train_labels)
train_seq_l  = tokenizer.texts_to_sequences(train_labels) 
val_seq_l = tokenizer.texts_to_sequences(val_labels)

tag2ind = tokenizer.word_index
tag2ind["Other"] = len(tag2ind)
print(tag2ind)
print(train_seq_l[:4])
#padding to prepare sequences of same length
train_seq_pad_l = pad_sequences(train_seq_l, maxlen=100, value = tag2ind["Other"],padding="post")
val_seq_pad_l = pad_sequences(val_seq_l, maxlen=100,value = tag2ind["Other"],padding="post")

print(train_seq_pad_l[:4])

print(train_seq_pad_l.shape)

# Randomly create your own word embeddings from scratch
embedding_matrix = np.zeros((n_unique, word_vec_size))
for word, i in word2ind.items():
  if i < n_unique:
    embedding_vector = np.random.rand(word_vec_size)*2 - 1
    if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector
#example 
print(embedding_matrix[3])

# You can check https://radimrehurek.com/gensim/models/word2vec.html for training a word embeddings from scratch

# You can check https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html and https://github.com/RaRe-Technologies/gensim-data for loading pretrained word embeddings.

# Word2Vec 
model_w2v = Word2Vec(train_words, size = word_vec_size, window = 10, workers = 10, min_count = 2)
vocs = list(model_w2v.wv.vocab)
print(vocs)
print(len(vocs))

num_words = len(vocs)
embedding_matrix_w2v = np.zeros((len(word2ind), word_vec_size))
for word, i in word2ind.items():
  if i < num_words:
    if word in model_w2v:
      embedding_vector = model_w2v[word]
      embedding_matrix_w2v[i] = embedding_vector
#example 
print(embedding_matrix_w2v[139])

# Embedding matrix with gensim api 
import gensim.downloader as api
model_gensim = api.load("glove-wiki-gigaword-100")

embedding_matrix_gensim = np.zeros((len(word2ind), word_vec_size))
for word, i in word2ind.items():
  if i < len(word2ind):
    if word in model_gensim:
      embedding_matrix_gensim[i] = model_gensim[word]
print(embedding_matrix_gensim[4])

# Create Embedding Matrices and Layers
# architecture from https://medium.com/analytics-vidhya/named-entity-recognition-using-deep-learning-elmo-embedding-bi-lstm-48295bc66cab
input_shape = Input(shape = (len(train_seq_pad[0]),))

max_len = max([len(x) for x in train_seq])
tag_len = len(tag2ind)
#embedding layer
embedding_layer = Embedding(
    len(word2ind),
    word_vec_size,
    weights=[embedding_matrix],
    input_length=max_len,
    trainable=False
  ) (input_shape)
x = Bidirectional(LSTM(units=128, return_sequences=True,
                       recurrent_dropout=0.2, dropout=0.2))(embedding_layer)
#x_rnn = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x_rnn)
#x = add([x, x_rnn])  # residual connection to the first biLSTM
x = Dense(64,activation = "sigmoid")(x)

out = TimeDistributed(Dense(tag_len, activation="softmax"))(x)

embedding_matrix.shape

embedding_matrix_w2v.shape

embedding_matrix_gensim.shape

# ****** Hyperparameter Tuning ******
import itertools
matrices=[embedding_matrix, embedding_matrix_w2v, embedding_matrix_gensim]
matrix_inds=[0,1,2]
dense_ns = [64, 128]
lstm_ns = [128, 64]

a=[matrix_inds,dense_ns, lstm_ns]

accs = []
combinations = []
models = []
max_model = 0
max_acc = 0
count = 0

input_shape = Input(shape = (len(train_seq_pad[0]),))

max_len = max([len(x) for x in train_seq])
tag_len = len(tag2ind)

for comb in list(itertools.product(*a)):

  embedding_layer = Embedding(
      len(word2ind),
      word_vec_size,
      weights=[ matrices[comb[0]] ],
      input_length=max_len,
      trainable=False
    ) (input_shape)
  x = Bidirectional(LSTM(units=comb[2], return_sequences=True,
                        recurrent_dropout=0.2, dropout=0.2))(embedding_layer)
  #x_rnn = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x_rnn)
  #x = add([x, x_rnn])  # residual connection to the first biLSTM
  x = Dense(comb[1],activation = "sigmoid")(x)

  out = TimeDistributed(Dense(tag_len, activation="softmax"))(x)
  model = Model(input_shape, out)
  model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
  #print(model.summary())

  model_res = model.fit(train_seq_pad, train_seq_pad_l, validation_data = (val_seq_pad,val_seq_pad_l) , batch_size = 500, epochs = 4, verbose = 1)
  val_acc = model_res.history["val_accuracy"][-1]
  print("Validation accuracy for ",comb, " is: ", val_acc)
  accs.append(val_acc)
  models.append(model)
  combinations.append(comb)
  if val_acc > max_acc:
    max_model = count
  count+=1

print("Properties of the model with highest accuracy:\n")
m = ["Scratch","word2vec","gensim"]
print("Embedding matrix:",m[combinations[max_model][0]])
print("Dense Layer Size:",combinations[max_model][1])
print("LSTM size:",combinations[max_model][2])
print("Validaiton accuracy", accs[max_model])

# Prepare test data
tokenizer = Tokenizer()

test_words = [[j[0] for j in w] for w in test_data ]
print(test_words[:4])
tokenizer.fit_on_texts(test_words)
test_seq  = tokenizer.texts_to_sequences(test_words) 

#padding to prepare sequences of same length
test_seq_pad  = pad_sequences(test_seq, maxlen=100,padding="post")

word_vec_size = 100

word2ind = tokenizer.word_index
n_unique = len(word2ind)
#print(train_seq_pad[:4])

#padding the labels 
tokenizer = Tokenizer()

test_labels = [[j[3] for j in w] for w in test_data ]
#print(train_labels[:4])
tokenizer.fit_on_texts(test_labels)
test_seq_l  = tokenizer.texts_to_sequences(test_labels) 

tag2ind = tokenizer.word_index
tag2ind["Other"] = len(tag2ind)
print(tag2ind)
print(test_seq_l[:4])
#padding to prepare sequences of same length
test_seq_pad_l = pad_sequences(test_seq_l, maxlen=100, value = tag2ind["Other"],padding="post")

print(test_seq_pad_l[:4])

# Prediction

preds = models[max_model].predict(test_seq_pad)
preds

preds[1,0]

#preds[0][0]-> word ner prob
preds_maxed = np.zeros(preds.shape[0:2])
for iy, ix,iz in np.ndindex(preds.shape):
  preds_maxed[iy,ix] = preds[iy,ix].argmax()
print(preds_maxed[0,1])

# define a function to remove paddings and align labels and tokens
def align_predictions(predictions:np.array):
  pr = list(predictions)

  for j in range(len(pr)):
    for k in range(len(pr[x])):
      pr[j][k] = np.delete(predictions[j], np.where(predictions[j] == 9))
  for iy, ix in np.ndindex(predictions.shape):
    predictions[iy] = np.delete(predictions[iy], np.where(predictions[iy] == 9))
  return predictions

preds_aligned = align_predictions(preds_maxed)

# Evaluate your models with functions of seqeval library

# ....

"""## My Report


In this project we had to perform named entity recognition based on two approaches, namely Conditional Random Fields and Recurrent Neural Networks. The data that we have utilized for this task is from Reuters and consists of various articles in English, that specifically contain various names of persons, organizations, locations, and miscellaneous entities.

All the data were presented as words with their tags in the format (word, postag, chunktag, ner tag). The training data consisted of 14987 words that were tagged in the same format, the validation had 3466 samples and the test set had 3684. 

Also to complement the tagging process, as part of the CRF stages, we have utilized a wide source of Wikipedia articles and fetched links that contained named entities. There were more than 25000 wikipedia articles and for each of them, using regular expression, the titles of pages that contained information regarding named entities were fetched and a gazetteer of size 244670 was created, to be used as a feature for the CRF tagging.

The CRF taggin part utilized the features that were identified by iterating over the sets and identifying values for all features. Some example features were: 
- Stem
- POS tag
- Chunk tag
- Start of the Sentence
- End of the Sentence
- Starts with an uppercase letter
- 𝑤𝑖’s shape
- 𝑤𝑖’s short word shape
- 𝑤𝑖 contains a number

Besides such examples, we have also repeated some features for the words that were to the left and right of the word that was being inspected.
To test the CRF model, we have one by one included the features, building up the list to its fullest. The top 3 scoring models were the last three additions, namely:

| Feature Name | F1 | Precision       | Recall |
|------------|-------------|--------------|----------|
| Stem     | 0.979061 | 0.979007    | 0.979255    | 
| Left-posttag     | 0.979535    | 0.979473 | 0.979720   | 
| Uppercase      | 0.979754 | 0.979678   | 0.979972   | 


For the recurrent neural models, we have utilized 3 different embedding matrices to fill our word embedding layer, to ease the identificaiton of relation between words. First one was randomly initialized with values between -1 and 1. The second one utilized a word corpus that was made from scratch using our own training data with the help of Word2Vec function. The last one was using pretrained models from the Gensim library to match and extract embedding vectors with our tokens in our corpus. These weights were then connected to an RNN before outputting.  
The architecture was a Bidirectional LSTM layer that operates on an embedding layer as previously mentioned, followed by a dense layer (fully connected) and then another dense layer that performed the label outputting but was wrapped in a Time Distributed layer, so that it updated itself at every processed word, which is a prominent feature of a recurrent neural network in essence.

The labels were preprocessed and we switched ner tags with index values, also creating a 9th label to put in place of the padded empty places. 

The hyperparameter tuning was performed using all of the embedding matrix types. Different hyperparameters tried were as follows: 

LSTM size = [128, 64],

dense layer node size = [64,128].

<br>

> **Properties of the model with highest accuracy:**
- Embedding matrix: gensim
- Dense Layer Size: 128
- LSTM size: 64
- Validaiton accuracy 0.9644172191619873


The best model was used to predict the labels and then accuracy metrics were performed, eliminating the previously created extra label. 
 
"""